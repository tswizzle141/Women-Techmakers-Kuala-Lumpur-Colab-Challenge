{"cells":[{"metadata":{"_uuid":"04ea043d76780d5dd39f8d6673b9d6b850b8bdff","_cell_guid":"d8437be7-1af2-4056-9c57-1c751ec71b79","id":"6uznQ7thrODO","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch.autograd import Variable\nfrom torch.optim import SGD\nfrom torchvision import models, transforms\nfrom keras.applications.inception_v3 import InceptionV3\nimport PIL\nimport os\nimport matplotlib\nfrom matplotlib import pyplot as plt\nfrom matplotlib import animation\nfrom IPython.display import HTML\nimport scipy.ndimage as ndimage\n%matplotlib inline\nimport scipy.ndimage as nd\nimport PIL.Image\nfrom IPython.display import clear_output, Image, display\nfrom io import BytesIO\n\ndef showarray(a, fmt='jpeg'):\n    a = np.uint8(np.clip(a,0,255)) #limit values in array\n    f = BytesIO()\n    PIL.Image.fromarray(a).save(f, fmt)\n    display(Image(data=f.getvalue()))\n    \ndef showtensor(a):\n    mean = np.array([0.485,0.456,0.406]).reshape([1,1,3])\n    std = np.array([0.229,0.224,0.225]).reshape([1,1,3])\n    inp = a[0,:,:,:]\n    inp = inp.transpose(1,2,0)\n    inp = std*inp+mean\n    inp *= 255\n    showarray(inp)\n    clear_output(wait=True)\n\ndef plot_images(im, titles=None):\n    plt.figure(figsize=(30,20))\n    \n    for i in range(len(im)):\n        plt.subplot(10/5+1,5,i+1)\n        plt.axis('off')\n        if titles is not None:\n            plt.title(titles[i])\n        plt.imshow(im[i])\n        \n    plt.pause(0.001)\n    \nnormalise = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])])\n\nnormalise_resize = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor(), transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])])\n\ndef init_image(size=(400,400,3)):\n    img = PIL.Image.fromarray(np.uint8(np.full(size,150)))\n    img = PIL.Image.fromarray(np.uint8(np.random.uniform(150,180,size)))\n    img_tensor = normalise(img).unsqueeze(0) #np.unsqueeze() = np.expand_dims() \n    img_np = img_tensor.numpy()\n    return img, img_tensor, img_np\n\ndef load_image(path, resize=False, size=None):\n    img = PIL.Image.open(path)\n    \n    if size is not None:\n        img.thumbnail(size, PIL.Image.ANTIALIAS)    \n    if resize:\n        img_tensor = normalise_resize(img).unsqueeze(0)\n    else:\n        img_tensor = normalise(img).unsqueeze(0)\n    img_np = img_tensor.numpy()\n    return img, img_tensor, img_np\n\ndef tensor_to_img(t):\n    a = t.numpy()\n    mean = np.array([0.485,0.456,0.406]).reshape([1,1,3])\n    std = np.array([0.229,0.224,0.225]).reshape([1,1,3])\n    inp = a[0,:,:,:]\n    inp = inp.transpose(1,2,0)\n    inp = std*inp+mean\n    inp *= 255\n    inp = np.uint8(np.clip(inp,0,255))\n    return PIL.Image.fromarray(inp)\n\ndef image_to_variable(image, requires_grad=False, cuda=False):\n    if cuda:\n        image = Variable(image.cuda(), requires_grad=requires_grad)\n    else:\n        image = Variable(image, requires_grad=requires_grad)\n    return image\n\ndef octaver_fn(model, base_img, step_fn, octave_n=6, octave_scale=1.4, iter_n=10, **step_args):\n    octaves = [base_img]\n    \n    for i in range(octave_n-1):\n        octaves.append(nd.zoom(octaves[-1], (1,1,1.0/octave_scale, 1.0 / octave_scale), order=1))\n\n    detail = np.zeros_like(octaves[-1])\n    for octave, octave_base in enumerate(octaves[::-1]):\n        h, w = octave_base.shape[-2:]\n        \n        if (octave>0):\n            h1, w1 = detail.shape[-2:]\n            detail = nd.zoom(detail, (1,1,1.0*h/h1, 1.0*w/w1), order=1)\n        \n        src = octave_base+detail\n        \n        for i in range(iter_n):\n            src = step_fn(model, src, **step_args)\n\n        detail = src.numpy()-octave_base\n\n    return src\n\ndef filter_step(model, img, layer_index, filter_index, step_size=5, display=True, use_L2=False):\n    global use_gpu\n    \n    mean = np.array([0.485,0.456,0.406]).reshape([3,1,1])\n    std = np.array([0.229,0.224,0.225]).reshape([3,1,1])\n    \n    model.zero_grad()\n    \n    img_var = image_to_variable(torch.Tensor(img), requires_grad=True, cuda=use_gpu)\n    optimizer = SGD([img_var], lr=step_size, weight_decay=1e-4)\n    \n    x = img_var\n    for index, layer in enumerate(model.features):\n        x = layer(x)\n        if index == layer_index:\n            break\n\n    output = x[0, filter_index]\n    loss = output.norm() #torch.mean(output)\n    loss.backward()\n    \n    if use_L2:\n        #L2 normalization on gradients\n        mean_square = torch.Tensor([torch.mean(img_var.grad.data**2) + 1e-5])\n        if use_gpu:\n            mean_square = mean_square.cuda()\n        img_var.grad.data /= torch.sqrt(mean_square)\n        img_var.data.add_(img_var.grad.data*step_size)\n    else:\n        optimizer.step()\n    \n    result = img_var.data.cpu().numpy()\n    result[0,:,:,:] = np.clip(result[0,:,:,:], -mean/std, (1-mean)/std)\n    \n    if display:\n        showtensor(result)\n    \n    return torch.Tensor(result)\n\ndef visualize_filter(model, base_img, layer_index, filter_index, octave_n=6, octave_scale=1.4, iter_n=10, step_size=5, display=True, use_L2=False):\n    \n    return octaver_fn(model, base_img, step_fn=filter_step, octave_n=octave_n, octave_scale=octave_scale, iter_n=iter_n, layer_index=layer_index, filter_index=filter_index, step_size=step_size, display=display, use_L2=use_L2)\n\ndef show_layer(layer_num, filter_start=10, filter_end=20, step_size=7, use_L2=False):\n    filters = []\n    titles = []\n    \n    _, _, img_np = init_image(size=(600,600,3))\n    for i in range(filter_start, filter_end):\n        title = \"Layer {} Filter {}\".format(layer_num, i)\n        print(title)\n        filter = visualize_filter(model, img_np, layer_num, filter_index=i, octave_n=2, iter_n=20, step_size=step_size, display=True, use_L2=use_L2)\n        filter_img = tensor_to_img(filter)\n        filter_img.save(title+\".jpg\")\n        filters.append(tensor_to_img(filter))\n        titles.append(title)\n        \n    plot_images(filters, titles)\n    return filters, titles\n\ndef objective(dst, guide_features):\n    if guide_features is None:\n        return dst.data\n    else:\n        x = dst.data[0].cpu().numpy()\n        y = guide_features.data[0].cpu().numpy()\n        ch, w, h = x.shape\n        x = x.reshape(ch,-1)\n        y = y.reshape(ch,-1)\n        A = x.T.dot(y)\n        diff = y[:, A.argmax(1)]\n        if use_gpu:\n            diff = torch.Tensor(np.array([diff.reshape(ch, w, h)])).cuda()\n        else: \n            diff = torch.Tensor(np.array([diff.reshape(ch, w, h)])) \n        return diff\n\ndef make_step(model, img, objective=objective, control=None, step_size=1.5, end=28, jitter=32):\n    global use_gpu\n    \n    mean = np.array([0.485,0.456,0.406]).reshape([3,1,1])\n    std = np.array([0.229,0.224,0.225]).reshape([3,1,1])\n    \n    ox, oy = np.random.randint(-jitter, jitter+1, 2)\n    \n    img = np.roll(np.roll(img, ox, -1), oy, -2)\n    tensor = torch.Tensor(img) \n    \n    img_var = image_to_variable(tensor, requires_grad=True, cuda=use_gpu)\n    model.zero_grad()\n      \n    x = img_var\n    for index, layer in enumerate(model.features.children()):\n        x = layer(x)\n        if index == end:\n            break\n    \n    delta = objective(x, control)\n    x.backward(delta)\n    \n    #L2 Regularization on gradients\n    mean_square = torch.Tensor([torch.mean(img_var.grad.data**2)])\n    if use_gpu:\n        mean_square = mean_square.cuda()\n    img_var.grad.data /= torch.sqrt(mean_square)\n    img_var.data.add_(img_var.grad.data*step_size)\n    \n    result = img_var.data.cpu().numpy()\n    result = np.roll(np.roll(result, -ox, -1), -oy, -2)\n    result[0,:,:,:] = np.clip(result[0,:,:,:], -mean/std, (1-mean)/std)\n    showtensor(result)\n    \n    return torch.Tensor(result)\n                                                             \ndef deepdream(model, base_img, octave_n=6, octave_scale=1.4, iter_n=10, end=28, control=None, objective=objective, step_size=1.5, jitter=32):\n    \n    return octaver_fn(model, base_img, step_fn=make_step, octave_n=octave_n, octave_scale=octave_scale, iter_n=iter_n, end=end, control=control, objective=objective, step_size=step_size, jitter=jitter)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"892421f03c631affbcf44b3c1b8eda01c974e78c","_kg_hide-input":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"inputImage = '../input/10-monkey-species/training/training/n2/n2133.jpg'\nmodel = models.densenet121() \nmodel.load_state_dict(torch.load(\"../input/densenet121/densenet121.pth\"))\n\nuse_gpu = False\nif torch.cuda.is_available():\n    use_gpu = True\n\nfor param in model.parameters():\n    param.requires_grad = False\n\nif use_gpu:\n    model.cuda()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"863fb94188c95aef2aa6f3bbb786f697eab8ed43","_cell_guid":"149ca97f-00b7-43c3-a8e0-871f8efe77d3","executionInfo":{"elapsed":4410,"status":"ok","timestamp":1526053309796,"user":{"photoUrl":"//lh5.googleusercontent.com/-_sBZsyc315U/AAAAAAAAAAI/AAAAAAAAAgg/b4D9SE9jgD0/s50-c-k-no/photo.jpg","userId":"107843268563316278814","displayName":"Carlo Alberto"},"user_tz":-120},"outputId":"bd9f48b8-c66a-43fc-a987-f7d31920af0b","id":"XZB9hS2zyVLt","trusted":true,"_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"input_img, input_tensor, input_np = load_image(inputImage, size=[1024, 1024])\ninput_img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"dream = deepdream(model, input_np, end=7, step_size=0.07, octave_n=6)\ndream = tensor_to_img(dream)\ndream.save('densenet121dream.jpg')\ndream","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"604dc88c4597b3bb389dbf8dbefe84325ccc1fed","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"inputImage = '../input/10-monkey-species/training/training/n2/n2133.jpg'\nmodel = models.vgg16() \nmodel.load_state_dict(torch.load(\"../input/vgg16/vgg16.pth\"))\nuse_gpu = False\nif torch.cuda.is_available():\n    use_gpu = True\nfor param in model.parameters():\n    param.requires_grad = False\nif use_gpu:\n    model.cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93577f6978f07aeff7db33d7531d0a96cf9bc381","collapsed":true},"cell_type":"code","source":"dream = deepdream(model, input_np, end=28, step_size=0.1, octave_n=6)\ndream = tensor_to_img(dream)\ndream.save('vgg16_dream.jpg')\ndream","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}